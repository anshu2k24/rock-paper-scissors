# ğŸª¨ğŸ’„âœ‚ï¸ Rock Paper Scissors Detection (YOLOv8)

A **YOLO-based image detection model** that identifies **rock, paper, and scissors** hand gestures from images or videos.
Trained on a **Kaggle Rock-Paper-Scissors dataset**, this project demonstrates real-time classification using modern computer vision techniques.

---

## ğŸš€ Features

* ğŸ¯ **YOLOv8 / YOLO11 models** fine-tuned for hand gesture detection
* ğŸ§  Detects **rock**, **paper**, and **scissors** postures from images or video streams
* ğŸ“¸ Compatible with **OpenCV** for live camera or batch image inference
* ğŸ§© Ready-to-extend for integration into a **gesture-based game engine**

---

## ğŸ§® Tech Stack

* **Python 3.10+**
* **Ultralytics YOLOv8**
* **OpenCV**
* **PyTorch**
* **NumPy / Pandas**

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ runs/detect/         # YOLO detection outputs
â”œâ”€â”€ test.py              # Test script for inference
â”œâ”€â”€ yolov8n.pt           # YOLOv8 model weights
â”œâ”€â”€ yolo11n.pt           # Experimental YOLO11 model weights
â””â”€â”€ README.md
```

---

## ğŸ¤ª Usage

### 1ï¸âƒ£ Install Dependencies

```bash
pip install ultralytics opencv-python torch
```

### 2ï¸âƒ£ Run Detection

Run inference on a folder of images:

```bash
python test.py --source path/to/images
```

Or test a single image:

```bash
python test.py --source sample.jpg
```

### 3ï¸âƒ£ Output

Detected results (with bounding boxes + labels) are saved in:

```
runs/detect/
```

---

## ğŸ§  Model Info

* **Base Model:** YOLOv8n / YOLO11n
* **Dataset:** Kaggle Rockâ€“Paperâ€“Scissors Dataset
* **Classes:** `['rock', 'paper', 'scissors']`

---

## ğŸ”® Future Plans

* ğŸ•¹ï¸ Add gameplay logic for AI vs Player
* ğŸŒ Build a Next.js dashboard for visual inference
* ğŸ“Š Add live webcam integration and analytics view

---

## ğŸ‘¨â€ğŸ’» Author

**Anshuman Pati**
[ğŸ”— GitHub](https://github.com/anshu2k24) | [ğŸ’¼ LinkedIn](https://www.linkedin.com/in/anshuman-pati-5575bb34a/)

---

> âš¡ *â€œWhere Computer Vision meets classic gameplay.â€*
